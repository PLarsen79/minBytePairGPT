{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a byte-pair-level GPT on some text data\n",
    "\n",
    "The inputs here are simple text files, which we chop up to byte pair chunks and then train GPT on. So you could say this is a byte-pair-transformer instead of a byte-pair-rnn. Doesn't quite roll off the tongue as well or what not. In this example we will feed it some shakespear, which we'll get it to predict.\n",
    "\n",
    "(Forked from Andrej karpathy's minGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def split_word(word, length=2):\n",
    "    if len(word) > length:\n",
    "        return (word[n:n+length] for n in range(0, len(word), length))\n",
    "    else:\n",
    "        if len(word) > 0:\n",
    "            return (word[n:n+len(word)] for n in range(0, len(word), len(word)))\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "# Byte-Pair enabled\n",
    "# Multiple token per bytepair\n",
    "class BytePairDataset(Dataset):\n",
    "\n",
    "    def __init__(self, fp, block_size, is_charlevel=True, bMultipleIns=False, bMultipleOuts=False, maxlines=10000):\n",
    "        \n",
    "        line = fp.readline()\n",
    "        self.chars = set([])\n",
    "        data = []\n",
    "        self.bMultipleIns = bMultipleIns\n",
    "        self.bMultipleOuts = bMultipleOuts\n",
    "        totlinec = 0\n",
    "        \n",
    "        while line and totlinec<maxlines:\n",
    "            lines = line\n",
    "            newline = \"dummy\"\n",
    "            linec = 0\n",
    "            while newline and linec<10000 and totlinec<maxlines:\n",
    "                newline = fp.readline()\n",
    "                if newline:\n",
    "                    lines += newline\n",
    "                linec += 1\n",
    "            \n",
    "            totlinec += linec\n",
    "            print(f\"Line {totlinec}\")\n",
    "            \n",
    "            if is_charlevel:\n",
    "                if bMultipleIns or bMultipleOuts:\n",
    "                    ch = list(set(lines))\n",
    "                    ch1 = list(set(lines))\n",
    "\n",
    "                    ch2 = [item + \"§2§\" for item in ch]\n",
    "                    ch3 = [item + \"§3§\" for item in ch]\n",
    "                    ch4 = [item + \"§4§\" for item in ch]\n",
    "                    ch5 = [item + \"§5§\" for item in ch]\n",
    "                    self.chars = self.chars.union(set(ch1+ch2+ch3+ch4+ch5))\n",
    "                else:\n",
    "                    self.chars = self.chars.union(list(set(lines)))\n",
    "            else:\n",
    "                ch1 = [item for word in lines.split(\"[NL]\") for item in split_word(word)]\n",
    "                if bMultipleIns or bMultipleOuts:\n",
    "                    ch1 += [item + \"§2§\" for word in lines.split(\"[NL]\") for item in split_word(word)]\n",
    "                    ch1 += [item + \"§3§\" for word in lines.split(\"[NL]\") for item in split_word(word)]\n",
    "                    ch1 += [item + \"§4§\" for word in lines.split(\"[NL]\") for item in split_word(word)]\n",
    "                    ch1 += [item + \"§5§\" for word in lines.split(\"[NL]\") for item in split_word(word)]\n",
    "                    self.chars = self.chars.union(set(ch1))\n",
    "                else:\n",
    "                    self.chars = self.chars.union(set(ch1))\n",
    "            \n",
    "            if is_charlevel:\n",
    "                data += lines\n",
    "            else:\n",
    "                data += [item for word in lines.split(\"[NL]\") for item in split_word(word)]\n",
    "            line = fp.readline()\n",
    "            \n",
    "        self.chars = sorted(self.chars)\n",
    "            \n",
    "        data_size, vocab_size = len(data), len(self.chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "        \n",
    "            \n",
    "    def stoimul(self, chunk):\n",
    "        counters = {}\n",
    "        imul = []\n",
    "        for s in chunk:\n",
    "            idx = self.stoi[s]\n",
    "            if idx in counters:\n",
    "                counters[idx] = min(5, counters[idx] + 1)\n",
    "            else:\n",
    "                counters[idx] = 1\n",
    "\n",
    "            imul.append(idx + counters[idx] - 1)\n",
    "        return imul\n",
    "    \n",
    "    def stoiouts(self, chunk):\n",
    "        iouts = []\n",
    "        for s in chunk:\n",
    "            idx = self.stoi[s]\n",
    "            #iouts.append(idx + random.randint(0, 4))\n",
    "            iouts.append(idx)\n",
    "        iouts[len(iouts)-1] = idx\n",
    "        return iouts\n",
    "    \n",
    "    def stoimulOuts(self, alldx):\n",
    "        iout = []\n",
    "        idx = alldx[len(alldx)-1]\n",
    "        for s in range(0, 5):\n",
    "            irow = []\n",
    "            nidx = idx + s\n",
    "            for s2 in range(1, len(alldx)-1):\n",
    "                iidx = alldx[s2]\n",
    "                irow.append(iidx)\n",
    "             \n",
    "            irow.append(nidx)\n",
    "            iout.append(torch.tensor(irow, dtype=torch.long))\n",
    "                \n",
    "        return iout\n",
    "    \n",
    "    def __len__(self):\n",
    "        #return math.ceil(len(self.data) / (self.block_size*2 + 1))\n",
    "        return math.ceil(len(self.data) - (self.block_size + 1))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        \n",
    "        z = None\n",
    "        if self.bMultipleIns:\n",
    "            dix = self.stoimul(chunk)\n",
    "        else:\n",
    "            if self.bMultipleOuts:\n",
    "                dix = self.stoiouts(chunk)\n",
    "            else:\n",
    "                dix = [self.stoi[s] for s in chunk]\n",
    "            \n",
    "        if self.bMultipleOuts:\n",
    "            z = self.stoimulOuts(dix)\n",
    "        \n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "\n",
    "        return x, y, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toSE(strSE): # Function to remove multiple token instances markup from output\n",
    "    strSE = strSE.replace(\"§2§\", \"\")\n",
    "    strSE = strSE.replace(\"§3§\", \"\")\n",
    "    strSE = strSE.replace(\"§4§\", \"\")\n",
    "    strSE = strSE.replace(\"§5§\", \"\")\n",
    "    return strSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 32 # spatial extent of the model for its context\n",
    "is_charlevel = False # True = tokens is per character, False = tokens is corpus split by bytepair chunks\n",
    "bMultipleIns = False # Multiple incremental instances for each token?\n",
    "bMultipleOuts = True # Multiple output instances for each token?\n",
    "bDecisionTreeLayers = True\n",
    "modelcount = 4 # Amount of cooperative models to train in parallell\n",
    "internalModelCount = 1 # Amount of parallell models internal for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 10000\n",
      "Line 20000\n",
      "Line 30000\n",
      "Line 39997\n",
      "data has 557697 characters, 6670 unique.\n"
     ]
    }
   ],
   "source": [
    "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
    "fp = open('input.txt', encoding=\"utf8\", mode='r') # don't worry we won't run out of file handles\n",
    "maxlines = 100000 # Limit, read the first 100.000 lines only\n",
    "train_dataset = BytePairDataset(fp, block_size, is_charlevel, bMultipleIns, bMultipleOuts, maxlines) # one line of poem is roughly 50 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2020 22:45:06 - INFO - mingpt.model -   number of parameters: 9.741824e+06\n",
      "10/07/2020 22:45:06 - INFO - mingpt.model -   number of parameters: 9.741824e+06\n",
      "10/07/2020 22:45:06 - INFO - mingpt.model -   number of parameters: 9.741824e+06\n",
      "10/07/2020 22:45:06 - INFO - mingpt.model -   number of parameters: 9.741824e+06\n",
      "10/07/2020 22:45:06 - INFO - mingpt.model -   number of parameters: 9.741824e+06\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, True,\n",
    "                  n_layer=8, n_head=8, n_embd=256, internalModelCount=internalModelCount)\n",
    "\n",
    "decmconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, False,\n",
    "                  n_layer=8, n_head=8, n_embd=256, internalModelCount=1)\n",
    "\n",
    "models=[]\n",
    "decmodel = GPT(decmconf)\n",
    "for k in range(modelcount):\n",
    "    models.append(GPT(mconf))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 17426: tl 4.8571. brtl 4.4935. bestm: 3. bim: 515. lr 3.001352e-04: 100%|██████████| 17427/17427 [5:00:39<00:00,  1.04s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 0, trained iter: 5652\n",
      "Model: 1, trained iter: 3844\n",
      "Model: 2, trained iter: 4055\n",
      "Model: 3, trained iter: 3876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "# We can't preload batches because the input tokens for the models depends on the output from the branching model\n",
    "tconf = TrainerConfig(max_epochs=1, batch_size=32, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
    "                      num_workers=0, block_size = block_size, bMultipleOuts = bMultipleOuts)\n",
    "trainer = Trainer(models, decmodel, modelcount, internalModelCount, train_dataset, None, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 0, sampled iter: 544, train bias: 1000.0\n",
      "Model: 1, sampled iter: 478, train bias: 1000.0\n",
      "Model: 2, sampled iter: 512, train bias: 1000.0\n",
      "Model: 3, sampled iter: 466, train bias: 1000.0\n"
     ]
    }
   ],
   "source": [
    "# alright, let's sample some bytepair-level shakespear\n",
    "from mingpt.utils import sampleMultiModelProb\n",
    "\n",
    "context = \"O God,\"\n",
    "if is_charlevel==False:\n",
    "    context = [item for word in context.split(\"[NL]\") for item in split_word(word)]\n",
    "\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sampleMultiModelProb(models, decmodel, modelcount, internalModelCount, x, 2000, temperature=1.0, sample=True, top_k=10, bMultipleIns=bMultipleIns, bMultipleOuts=bMultipleOuts)[0]\n",
    "completionMultiModel = ''.join([train_dataset.itos[int(i)] for i in y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God,\n",
      "And bath not and, and by make sir we mane, not make of not the ther.\n",
      "\n",
      "ANTONONONONIO:\n",
      "And mave as not thy not to a mut, nod the not his costen should ther or not not me.\n",
      "\n",
      "ALLO:\n",
      "The he and make hour mane,\n",
      "Thes, our that the ban mine.\n",
      "\n",
      "ANSTPEBAPTANANIO:\n",
      "I cang trance com in the hath your as of shat and ing in am and of heess as thincent.\n",
      "\n",
      "ANTO:\n",
      "Not, you ind than and thou are hall won I was thim ing main the mar,\n",
      "When.\n",
      "\n",
      "PONSTTISTO:\n",
      "Shood, I be will not, bear thour the coster and ou make the wer that,\n",
      "And my marroonth with wit this you in the so ing of so the herest thich him say, the of so to sir, of have, will here thich of the to has, of bere to me ine, that's the he pine ing of and that not hich hand ance iu and of shat not my hear ine ing make\n",
      "Thicher an of he the now the thand to from is were there his make in me, ing ince withis cone,\n",
      "This or mond, ther, my from thour the and his maine the withe th thou theen hast on is of the in on of wor me mand din to me,\n",
      "And of an make, that my of but the fore to whave spest if her the that with his ince to sor thin too the he thing mand wor a fould my ther am ine the her hall a mouf me and her whis thou, of the dear his here, ant the in at of thonood,\n",
      "The and thous in me ing he we he wore ing may thand of wher in the shou that with to bing to to this and ot inest an on of there, I hour, not to spe.\n",
      "\n",
      "PROSTLOLO:\n",
      "And ing not thou wer, my did sheare you the ing is shaven in to ing is if no there.\n",
      "\n",
      "GLONTTILO:\n",
      "Thy hat here am ing me, thand thould for of in there.\n",
      "\n",
      "P:\n",
      "And you me, not to dot mand to to to would sour and thater of of must corth the the that so the hall bines,\n",
      "For ing theen not, to the no me have some thin and sir,\n",
      "And and thind ing thene main there dow of me.\n",
      "\n",
      "ANTIO:\n",
      "Hand in of would not sir of make and by may thou his or to a wout ine have and she dord thing of as the make she wour to his iters,\n",
      "No the con he ind me and him,\n",
      "Wher of not ince the me the thin to have, not the that but with with dave that of ther me thy he some.\n",
      "\n",
      "PRANSTIAIAAN:\n",
      "Ther, the ing the and conter, non mast of you mast our bess come we thand, all bear an the meach she more hink will no sour such and out wand and not a fave sher come.\n",
      "\n",
      "PROAPIO:\n",
      "Thand of mesere will whould of hour indess mand the ther.\n",
      "\n",
      "I havou speake had thy the of should her here shal not his alll whis here the wine ine ance,\n",
      "Now not of here, that there here of thing here sir,\n",
      "Thy of and and his he thy then and me of some mome so ind ther that's not thintel to a son, to his me could the have him, in the deter, for ther were hat,\n",
      "And this the the there,\n",
      "This that wing inter to some.\n",
      "\n",
      "BASTBAANHI:\n",
      "And hinke he sir, theer he whoust come the some ince, to say.\n",
      "\n",
      "The day, thou whing by the some, to see, he the dor thou may me and, to be this all and be now of an to was on thour her and thath hath some not ther,\n",
      "The and the the the may the mainde wit that the of some mang.\n",
      "\n",
      "PERO:\n",
      "\n",
      "PRIAood ant his and here.\n",
      "\n",
      "Pare you, havine ing thy more ir the and make bince, thou weld the cothee, now thee shat noth all to the move and me now me sir my to so heress,\n",
      "Whis mand hich as thour ine of as not, and dor and bine hathy hen of bore is woung of of lest the com this in the me if a mond we cove.\n",
      "\n",
      "ALLO:\n",
      "I wer thou to to thy sould that ban he may thee, in.\n",
      "\n",
      "And but not, thy worth hat ther of not the not the with thich and bing of bring,\n",
      "There ther will haver you to have marrin thou ing of dought he with he wor where,\n",
      "The this bind here sher the the there and to whis ing of by this was, ing ther weathe the no mut have thy core in the mane,\n",
      "And ing the not ing the in thould to what in are.\n",
      "\n",
      "PRONIANO:\n",
      "All, you of thill the so thee, you not to but the dain then her thichand he pur the sour here,\n",
      "Whis wour angess, my for.\n",
      "\n",
      "Woul of by withink not, wor, thing there, thou sess, I make, he dide hang.\n",
      "\n",
      "To me not some commore is in come spe, hipol of bring hen that me think wor and mear ther.\n",
      "\n",
      "Any shall to now of here, my make a whou here on of must an that ther to cove w\n"
     ]
    }
   ],
   "source": [
    "print(toSE(completionMultiModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well that was fun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
