{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a byte-pair-level GPT on some text data\n",
    "\n",
    "The inputs here are simple text files, which we chop up to byte pair chunks and then train GPT on. So you could say this is a byte-pair-transformer instead of a byte-pair-rnn. Doesn't quite roll off the tongue as well or what not. In this example we will feed it some shakespear, which we'll get it to predict.\n",
    "\n",
    "(Forked from Andrej karpathy's minGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def split_word(word, length=2):\n",
    "    if len(word) > length:\n",
    "        return (word[n:n+length] for n in range(0, len(word), length))\n",
    "    else:\n",
    "        if len(word) > 0:\n",
    "            return (word[n:n+len(word)] for n in range(0, len(word), len(word)))\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "# Byte-Pair enabled\n",
    "# Multiple token per bytepair\n",
    "class BytePairDataset(Dataset):\n",
    "\n",
    "    def __init__(self, fp, block_size, is_charlevel=True, bMultipleIns=False, bMultipleOuts=False, maxlines=10000):\n",
    "        \n",
    "        line = fp.readline()\n",
    "        self.chars = set([])\n",
    "        data = []\n",
    "        self.bMultipleIns = bMultipleIns\n",
    "        self.bMultipleOuts = bMultipleOuts\n",
    "        totlinec = 0\n",
    "        \n",
    "        while line and totlinec<maxlines:\n",
    "            lines = line\n",
    "            newline = \"dummy\"\n",
    "            linec = 0\n",
    "            while newline and linec<10000 and totlinec<maxlines:\n",
    "                newline = fp.readline()\n",
    "                if newline:\n",
    "                    lines += newline\n",
    "                linec += 1\n",
    "            \n",
    "            totlinec += linec\n",
    "            print(f\"Line {totlinec}\")\n",
    "            \n",
    "            if is_charlevel:\n",
    "                if bMultipleIns or bMultipleOuts:\n",
    "                    ch = list(set(lines))\n",
    "                    ch1 = list(set(lines))\n",
    "\n",
    "                    ch2 = [item + \"§2§\" for item in ch]\n",
    "                    ch3 = [item + \"§3§\" for item in ch]\n",
    "                    ch4 = [item + \"§4§\" for item in ch]\n",
    "                    ch5 = [item + \"§5§\" for item in ch]\n",
    "                    self.chars = self.chars.union(set(ch1+ch2+ch3+ch4+ch5))\n",
    "                else:\n",
    "                    self.chars = self.chars.union(list(set(lines)))\n",
    "            else:\n",
    "                ch1 = [item for word in lines.split(\"[NL]\") for item in split_word(word)]\n",
    "                if bMultipleIns or bMultipleOuts:\n",
    "                    ch1 += [item + \"§2§\" for word in lines.split(\"[NL]\") for item in split_word(word)]\n",
    "                    ch1 += [item + \"§3§\" for word in lines.split(\"[NL]\") for item in split_word(word)]\n",
    "                    ch1 += [item + \"§4§\" for word in lines.split(\"[NL]\") for item in split_word(word)]\n",
    "                    ch1 += [item + \"§5§\" for word in lines.split(\"[NL]\") for item in split_word(word)]\n",
    "                    self.chars = self.chars.union(set(ch1))\n",
    "                else:\n",
    "                    self.chars = self.chars.union(set(ch1))\n",
    "            \n",
    "            if is_charlevel:\n",
    "                data += lines\n",
    "            else:\n",
    "                data += [item for word in lines.split(\"[NL]\") for item in split_word(word)]\n",
    "            line = fp.readline()\n",
    "            \n",
    "        self.chars = sorted(self.chars)\n",
    "            \n",
    "        data_size, vocab_size = len(data), len(self.chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "        \n",
    "            \n",
    "    def stoimul(self, chunk):\n",
    "        counters = {}\n",
    "        imul = []\n",
    "        for s in chunk:\n",
    "            idx = self.stoi[s]\n",
    "            if idx in counters:\n",
    "                counters[idx] = min(5, counters[idx] + 1)\n",
    "            else:\n",
    "                counters[idx] = 1\n",
    "\n",
    "            imul.append(idx + counters[idx] - 1)\n",
    "        return imul\n",
    "    \n",
    "    def stoiouts(self, chunk):\n",
    "        iouts = []\n",
    "        for s in chunk:\n",
    "            idx = self.stoi[s]\n",
    "            #iouts.append(idx + random.randint(0, 4))\n",
    "            iouts.append(idx)\n",
    "        iouts[len(iouts)-1] = idx\n",
    "        return iouts\n",
    "    \n",
    "    def stoimulOuts(self, alldx):\n",
    "        iout = []\n",
    "        idx = alldx[len(alldx)-1]\n",
    "        for s in range(0, 5):\n",
    "            irow = []\n",
    "            nidx = idx + s\n",
    "            for s2 in range(1, len(alldx)-1):\n",
    "                iidx = alldx[s2]\n",
    "                irow.append(iidx)\n",
    "             \n",
    "            irow.append(nidx)\n",
    "            iout.append(torch.tensor(irow, dtype=torch.long))\n",
    "                \n",
    "        return iout\n",
    "    \n",
    "    def __len__(self):\n",
    "        #return math.ceil(len(self.data) / (self.block_size*2 + 1))\n",
    "        return math.ceil(len(self.data) - (self.block_size + 1))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        \n",
    "        z = None\n",
    "        if self.bMultipleIns:\n",
    "            dix = self.stoimul(chunk)\n",
    "        else:\n",
    "            if self.bMultipleOuts:\n",
    "                dix = self.stoiouts(chunk)\n",
    "            else:\n",
    "                dix = [self.stoi[s] for s in chunk]\n",
    "            \n",
    "        if self.bMultipleOuts:\n",
    "            z = self.stoimulOuts(dix)\n",
    "        \n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "\n",
    "        return x, y, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toSE(strSE): # Function to remove multiple token instances markup from output\n",
    "    strSE = strSE.replace(\"§2§\", \"\")\n",
    "    strSE = strSE.replace(\"§3§\", \"\")\n",
    "    strSE = strSE.replace(\"§4§\", \"\")\n",
    "    strSE = strSE.replace(\"§5§\", \"\")\n",
    "    return strSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 32 # spatial extent of the model for its context\n",
    "is_charlevel = False # True = tokens is per character, False = tokens is corpus split by \" \" and bytepair chunks\n",
    "bMultipleIns = False # Multiple incremental instances for each token?\n",
    "bMultipleOuts = True # Multiple output instances for each token?\n",
    "bDecisionTreeLayers = True\n",
    "modelcount = 4 # Amount of cooperative models to train in parallell\n",
    "internalModelCount = 1 # Amount of parallell models internal for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 10000\n",
      "Line 20000\n",
      "Line 30000\n",
      "Line 39997\n",
      "data has 557697 characters, 6670 unique.\n"
     ]
    }
   ],
   "source": [
    "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
    "fp = open('input.txt', encoding=\"utf8\", mode='r') # don't worry we won't run out of file handles\n",
    "maxlines = 100000 # Limit, read the first 100.000 lines only\n",
    "train_dataset = BytePairDataset(fp, block_size, is_charlevel, bMultipleIns, bMultipleOuts, maxlines) # one line of poem is roughly 50 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2020 22:24:06 - INFO - mingpt.model -   number of parameters: 9.741824e+06\n",
      "10/07/2020 22:24:06 - INFO - mingpt.model -   number of parameters: 9.741824e+06\n",
      "10/07/2020 22:24:07 - INFO - mingpt.model -   number of parameters: 9.741824e+06\n",
      "10/07/2020 22:24:07 - INFO - mingpt.model -   number of parameters: 9.741824e+06\n",
      "10/07/2020 22:24:07 - INFO - mingpt.model -   number of parameters: 9.741824e+06\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, True,\n",
    "                  n_layer=8, n_head=8, n_embd=256, internalModelCount=internalModelCount)\n",
    "\n",
    "decmconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, False,\n",
    "                  n_layer=8, n_head=8, n_embd=256, internalModelCount=1)\n",
    "\n",
    "models=[]\n",
    "decmodel = GPT(decmconf)\n",
    "for k in range(modelcount):\n",
    "    models.append(GPT(mconf))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 3: tl 8.8654. brtl 8.1044. bestm: 2. bim: 318. lr 2.400000e-03:   0%| | 4/17427 [00:13<16:05:42,  3.33s/it\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-04159537a9a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                       num_workers=0, block_size = block_size, bMultipleOuts = bMultipleOuts)\n\u001b[0;32m      8\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodelcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minternalModelCount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\Dokument\\minGPT-multiDecisionTreeModel16\\mingpt\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0mrun_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_dataset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Dokument\\minGPT-multiDecisionTreeModel16\\mingpt\\trainer.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[1;34m(split)\u001b[0m\n\u001b[0;32m    149\u001b[0m                         \u001b[0mdloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m                         \u001b[0mdecmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m                         \u001b[0mdloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_norm_clip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m                         \u001b[0moptimizerdec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "# We can't preload batches because the input tokens for the models depends on the output from the branching model\n",
    "tconf = TrainerConfig(max_epochs=1, batch_size=32, learning_rate=6e-3,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
    "                      num_workers=0, block_size = block_size, bMultipleOuts = bMultipleOuts)\n",
    "trainer = Trainer(models, decmodel, modelcount, internalModelCount, train_dataset, None, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# alright, let's sample some bytepair-level shakespear\n",
    "from mingpt.utils import sampleMultiModelProb\n",
    "\n",
    "context = \"O God,\"\n",
    "if is_charlevel==False:\n",
    "    context = [item for word in context.split(\"[NL]\") for item in split_word(word)]\n",
    "\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sampleMultiModelProb(models, decmodel, modelcount, internalModelCount, x, 2000, temperature=1.0, sample=True, top_k=10, bMultipleIns=bMultipleIns, bMultipleOuts=bMultipleOuts)[0]\n",
    "completionMultiModel = ''.join([train_dataset.itos[int(i)] for i in y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(toSE(completionMultiModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well that was fun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
